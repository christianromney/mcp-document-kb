#+TITLE: MCP Document Knowledge Base
#+AUTHOR: Christian Romney
#+DATE: 2025-07-08
#+STARTUP: overview
#+OPTIONS: toc:2 num:nil
#+PROPERTY: header-args :mkdirp yes
* Architectural Briefing
** What is it?

mcp-document-kb is a Clojure-based MCP (Model Context Protocol) server that provides semantic search capabilities for local knowledge bases built from Emacs org-mode notes. It enables intelligent retrieval and analysis of personal knowledge through vector embeddings and graph-based concept linking.

** What problem(s) does it strive to solve?

The primary problem is the limitation of traditional note-taking systems that rely on simple pattern matching (grep/ripgrep) for search, which fails to capture semantic relationships between concepts. This creates several interconnected challenges:

*** Knowledge Retrieval Limitations
Users cannot find relevant notes based on conceptual similarity - searching for "AI" won't surface notes about "machine learning" or "neural networks" despite their semantic relationship.

*** Context Fragmentation
When providing context to LLMs for summarization or analysis, relevant information remains scattered across multiple files without semantic connections, reducing the quality of AI-assisted knowledge work.

*** Knowledge Graph Blindness
Users cannot visualize or traverse the conceptual relationships within their knowledge base, missing opportunities to discover connections between ideas and identify knowledge gaps.

*** Manual Knowledge Management
Traditional systems require manual tagging, linking, and organization, creating maintenance overhead that discourages consistent knowledge capture and curation.

*** Privacy and Cost Concerns
Cloud-based solutions expose personal knowledge to external services, while local solutions lack the sophisticated search capabilities needed for effective knowledge retrieval.

** Core abstractions

The system is built around four fundamental abstractions:

*** Document Fragment
A semantically meaningful chunk of text extracted from org-mode files, representing the atomic unit of searchable knowledge with associated metadata (file path, timestamps, hierarchical context).

*** Semantic Vector
A high-dimensional numerical representation of document fragments generated by local embedding models, enabling similarity-based search that captures conceptual relationships beyond keyword matching.

*** Knowledge Concept
An extracted semantic topic or theme that represents a key idea within the knowledge base, forming the nodes of a conceptual graph with weighted relationships to other concepts.

*** MCP Tool
A standardized interface for exposing knowledge base capabilities to LLM clients, providing structured access to search, analysis, and graph traversal operations through the Model Context Protocol.

** Primary operations

The system exposes its capabilities through well-defined MCP tools that can be composed into complex knowledge workflows:

*** search-knowledge-base
Performs semantic similarity search across the knowledge base, accepting query strings and returning ranked document fragments with relevance scores and metadata. Supports filtering by date ranges, file types, and concept categories.

*** extract-topics
Analyzes document content to identify primary semantic topics and concepts, returning structured representations of key themes with confidence scores and relationships to existing knowledge graph nodes.

*** build-knowledge-graph
Constructs or updates conceptual relationships between documents and extracted topics, creating a navigable network of ideas with weighted edges representing semantic similarity and co-occurrence patterns.

*** get-document-context
Retrieves contextual information around specific document fragments, including hierarchical structure (org-mode headers), related fragments, and temporal context (creation/modification dates).

*** traverse-concept-graph
Navigates the knowledge graph starting from specific concepts or documents, enabling exploration of related ideas and discovery of unexpected connections within the knowledge base.

** Architectural Components

The system follows a layered architecture with clear separation of concerns:

#+BEGIN_SRC mermaid :file tangle/architecture-diagram.png :exports both
graph TB
    subgraph "Client Layer"
        A[LLM Tools] --> B[MCP Client]
        C[IDE Extensions] --> B
        D[Web Interface] --> B
    end

    subgraph "MCP Server Layer"
        B --> E[JSON-RPC Handler]
        E --> F[Tool Registry]
        F --> G[Session Manager]
        G --> H[Transport Layer]
    end

    subgraph "Application Layer"
        H --> I[Search Engine]
        H --> J[Knowledge Graph Builder]
        H --> K[Document Processor]

        I --> L[Query Processor]
        I --> M[Result Ranker]

        J --> N[Topic Extractor]
        J --> O[Concept Linker]

        K --> P[Org-mode Parser]
        K --> Q[Text Chunker]
        K --> R[Metadata Extractor]
    end

    subgraph "Storage Layer"
        L --> S[Qdrant Vector DB]
        M --> S
        N --> T[Knowledge Graph Store]
        O --> T
        R --> U[Document Metadata Store]
    end

    subgraph "External Services"
        Q --> V[Ollama Embedding API]
        N --> W[Ollama LLM API]
    end

    subgraph "File System"
        X[Org-mode Files] --> Y[File Watcher]
        Y --> K
    end
#+END_SRC

#+RESULTS:
[[file:tangle/architecture-diagram.png]]

*** Environmental Requirements
- Local Qdrant instance for vector storage and similarity search
- Local Ollama installation with embedding models (mxbai-embed-large recommended)
- Java 8+ runtime for Clojure execution
- File system access to org-mode document directories

*** Process Architecture
- Single JVM process with embedded HTTP server for MCP communication
- Asynchronous file watching using Java NIO WatchService
- Background thread pool for document processing and embedding generation
- Connection pooling for Qdrant and Ollama API interactions

** Is it simple?

The system prioritizes simplicity through several design decisions:

*** Stateless Design
The MCP server maintains no client session state, with all context provided through request parameters. This eliminates complex state management and supports horizontal scaling.

*** Functional Architecture
Core business logic is implemented as pure functions with clear inputs and outputs, making the system predictable and testable while avoiding hidden dependencies.

*** Minimal External Dependencies
The system depends only on essential external services (Qdrant, Ollama) that can be easily containerized and managed locally, avoiding complex distributed system concerns.

*** Declarative Configuration
System behavior is controlled through EDN configuration files with sensible defaults, eliminating the need for imperative setup procedures.

However, the system does introduce some complexity:

*** Stateful Components
The vector database and knowledge graph maintain persistent state that must be kept synchronized with the file system, requiring careful coordination and error handling.

*** Asynchronous Processing
File watching and document ingestion operate asynchronously, creating eventual consistency challenges that must be handled gracefully.

*** Multi-model Coordination
The system coordinates between embedding models, vector search, and knowledge graph operations, requiring careful orchestration to maintain performance and accuracy.

** Fundamental tradeoffs

*** Benefits

**** Semantic Search Capability
Trades simple keyword matching for sophisticated conceptual search, enabling users to find relevant knowledge based on meaning rather than exact terminology.

**** Privacy Preservation
Trades cloud-based convenience for local processing, ensuring personal knowledge remains private while maintaining full control over data and processing.

**** LLM Integration
Trades standalone operation for MCP protocol support, enabling seamless integration with AI tools while maintaining standardized interfaces.

**** Automatic Knowledge Management
Trades manual organization for automated concept extraction and linking, reducing maintenance overhead while improving knowledge discoverability.

*** Risks

**** Local Resource Requirements
Trades cloud scalability for local hardware constraints, requiring sufficient local compute and storage resources for embedding generation and vector search.

**** Model Dependency
Trades cloud API flexibility for local model limitations, constraining embedding quality and topic extraction capabilities to locally available models.

**** Eventual Consistency
Trades immediate consistency for asynchronous processing performance, creating potential delays between file changes and search availability.

**** Setup Complexity
Trades cloud service simplicity for local service management, requiring users to configure and maintain Qdrant and Ollama installations.

** Unknowns

*** Performance Characteristics
The system's behavior with large knowledge bases (>10,000 documents) remains untested, particularly regarding memory usage patterns, search latency, and knowledge graph traversal performance.

*** Embedding Model Quality
The effectiveness of locally available embedding models for domain-specific knowledge extraction compared to cloud-based alternatives requires empirical evaluation.

*** Knowledge Graph Scalability
The optimal strategies for managing knowledge graph complexity and avoiding over-connection as the knowledge base grows are not yet determined.

*** Multi-language Support
The system's capability to handle org-mode files in multiple languages and technical domains needs validation.

** Key indicators for use

*** Large Personal Knowledge Base
Organizations or individuals with substantial collections of org-mode notes (>1,000 documents) who require sophisticated search capabilities beyond simple text matching.

*** Privacy-Sensitive Content
Use cases involving confidential research, proprietary knowledge, or personal information where cloud-based solutions present unacceptable privacy risks.

*** LLM-Assisted Knowledge Work
Workflows that regularly provide knowledge base context to language models for summarization, analysis, or content generation.

*** Concept Discovery Requirements
Applications requiring identification of thematic relationships and conceptual connections within knowledge bases for research or analysis purposes.

*** Local Infrastructure Preference
Organizations with existing local infrastructure and technical expertise who prefer self-hosted solutions over cloud dependencies.

** Key indicators against use

*** Small Knowledge Collections
Knowledge bases with fewer than 100 documents may not benefit from semantic search capabilities, making the system's complexity unjustified.

*** Simple Search Requirements
Use cases satisfied by basic keyword search or where document organization already provides adequate discoverability.

*** Limited Technical Resources
Environments lacking the technical expertise to configure and maintain Qdrant, Ollama, and associated infrastructure components.

*** Cloud-First Strategies
Organizations with strong preferences for cloud-based solutions and existing investments in cloud knowledge management platforms.

*** Non-org-mode Content
Knowledge bases primarily composed of formats other than org-mode files, where the system's parsing capabilities provide limited value.

** Alternatives

*** Obsidian with Plugins
Provides graph-based knowledge management with some semantic capabilities, but lacks the deep MCP integration and local LLM support. Trades technical control for user-friendly interfaces.

*** Logseq
Offers local-first knowledge management with block-based organization and some AI integration. Provides better mobile support but less sophisticated semantic search capabilities.

*** Roam Research
Delivers powerful knowledge graph capabilities with real-time collaboration, but operates as a cloud service with privacy implications and subscription costs.

*** Elasticsearch with Custom Indexing
Provides industrial-strength search capabilities with custom semantic processing, but requires significant infrastructure investment and lacks knowledge graph features.

*** Notion AI
Offers AI-powered search and summarization with excellent user experience, but depends on cloud services and provides limited customization for specialized knowledge domains.

** Other relevant characteristics

*** Latency Profile
Search operations complete in <500ms for typical knowledge bases, with initial document ingestion requiring 1-2 seconds per document depending on embedding model performance.

*** Scalability Boundaries
The system effectively handles knowledge bases up to 50,000 documents with 8GB RAM, beyond which performance degrades due to vector search memory requirements.

*** Security Model
All processing occurs locally with no external network dependencies for core operations, supporting air-gapped environments and compliance requirements.

*** Extensibility Design
The plugin architecture supports custom document parsers, embedding models, and search algorithms, enabling adaptation to specialized knowledge domains.

*** Operational Simplicity
The system requires minimal ongoing maintenance once configured, with automatic handling of file changes and self-healing capabilities for temporary service outages.

** Resources

- [[https://modelcontextprotocol.io][Model Context Protocol Specification]] - Standard for LLM-tool communication
- [[https://qdrant.tech/documentation/][Qdrant Vector Database Documentation]] - Vector search implementation guide
- [[https://ollama.com/][Ollama Local LLM Platform]] - Local model management and API
- [[https://orgmode.org/manual/][Org-mode Documentation]] - Document format specification
- [[https://clojure.org/][Clojure Programming Language]] - Implementation language resources
- [[file:../PLAN.md][System Architecture Diagrams]] - Detailed implementation planning documentation
